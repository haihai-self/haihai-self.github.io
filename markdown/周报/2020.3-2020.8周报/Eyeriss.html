<p>[TOC]</p>
<h2 id="A-Spatial-Architecture-for-Energy-Efficient-Dataflow-for-Convolutional-Neural-Networks"><a href="#A-Spatial-Architecture-for-Energy-Efficient-Dataflow-for-Convolutional-Neural-Networks" class="headerlink" title="A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks"></a>A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks</h2><p>ISCA16’</p>
<h3 id="1-1-What-the-problems-the-paper-mentioned-intro"><a href="#1-1-What-the-problems-the-paper-mentioned-intro" class="headerlink" title="1.1 What the problems the paper mentioned ?(intro)"></a>1.1 What the problems the paper mentioned ?(intro)</h3><p>提出RS(rowstationary)架构，主要想解决的是神经网络中数据搬运耗能的问题。主要的思想是数据复用</p>
<ul>
<li>针对之前CNN的dataflow进行分类</li>
<li>RS，针对提高吞吐量跟能效的架构</li>
<li>分析框架，在相同的硬件约束下分析不同架构的能耗，同时能找到不同架构的最优能耗</li>
</ul>
<h3 id="1-2-Summary-of-major-innovations-intro"><a href="#1-2-Summary-of-major-innovations-intro" class="headerlink" title="1.2 Summary of major innovations (intro)"></a>1.2 Summary of major innovations (intro)</h3><p>该架构支持神经网络的并行计算并且少的数据移动。为了评价数据移动的能耗，将根据移动数据需要的能耗，分为不同的等级。</p>
<h3 id="1-3-What-are-some-intriguing-aspects-of-the-paper-design-amp-implementation"><a href="#1-3-What-are-some-intriguing-aspects-of-the-paper-design-amp-implementation" class="headerlink" title="1.3 What are some intriguing aspects of the paper?(design&amp;implementation)"></a>1.3 What are some intriguing aspects of the paper?(design&amp;implementation)</h3><p>high-level block diagram:</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1586871538.jpg" alt="1586871538"></p>
<p>主要组成是一个SA加速器跟一个片外DRAM，数据可以从CPU或者GPU加载进入DRAM，然后被SA调用进行计算，计算完成之后写回DRAM。</p>
<p>SA主要由Global Buffer 跟PE Array组成</p>
<ul>
<li>global buffer用于输入数据重用并隐藏DRAM访问延迟，典型的大小为100-300KB</li>
<li>PE Array由Noc连接，其中的ALU主要做的是乘加操作</li>
<li>register file (RF)主要保存的是中间结果，由于不同的数据大小不同，典型的大小是一个PE 1KB</li>
<li>PE FIFO (pFIFO)用于控制数据</li>
</ul>
<p>数据处理在MAC方面的问题</p>
<ul>
<li>一个ifmap其实很多数据是可以重用的，每次卷积都单独的加载数据的话会导致很大的bandwidth与energy的浪费</li>
<li>产生的部分和会导致浪费</li>
</ul>
<p><strong>EXISTING CNN DATAFLOWS</strong></p>
<ul>
<li><p>Weight Stationary (WS) Dataflow，就是权重输入固定到RF中，然后将ifmap不断的输入到相应的pe中。</p>
</li>
<li><p>Output Stationary (OS) Dataflow，固定ofmap，将psum保存在RF中</p>
</li>
<li><p>No Local Reuse (NLR) Dataflow，不是RF级别的数据重用，而是利用PE之间的数据通信进行ifmap与psum的reuse，不需要RF但是每个组需要一个buffer</p>
</li>
</ul>
<p>这些架构只是单纯的考虑了ifmap或者只考虑了filter，不能达到最高的效率</p>
<p><strong>ENERGY-EFFICIENT DATAFLOW: ROW STATIONARY</strong></p>
<p>filter height (R)</p>
<p>ofmap height (E)</p>
<p>ifmap/filter channels (C)</p>
<p>number of filters (M)</p>
<p> fmap batch size (N)</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1586920429.jpg" alt="1586920429"></p>
<p>将filter与ifmap映射到一维进行卷积</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1586921879.jpg" alt="1586921879"></p>
<p>2D convolution展开，filter进行行广播，ifmap进行斜对角广播，得出得psum在列向求和得到，由于输入跟特征可能很多，不能一次全部在PE Array中进行展开，因此需要分为两步，第一步为logical map，第二步为physical map。</p>
<p>physical mapping需要使用折叠技术，将物理单元进行分时复用</p>
<p>通过逻辑跟物理的映射，该结构实现了weight在横向上的reuse，input在斜向上方向的reuse，psum在纵向上实现了数据的reuse。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1586921996.jpg" alt="1586921996"></p>
<p><strong>Energy-Efficient Data Handling</strong></p>
<p>数据移动分为四个能级，由低到高分别为</p>
<ul>
<li>RF：从PE的寄存器取数</li>
<li>Array：使用PE之间的communication来进行数据的从用</li>
<li>Global Buffer：将数据存储到片上内存</li>
<li>DRAM：从片外取数</li>
</ul>
<h3 id="1-4-How-to-test-compare-analyze-the-results-experiment"><a href="#1-4-How-to-test-compare-analyze-the-results-experiment" class="headerlink" title="1.4 How to test/compare/analyze the results?(experiment)"></a>1.4 How to test/compare/analyze the results?(experiment)</h3><ul>
<li><p>Input Data Access Energy Cost:</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1587001513.jpg" alt="1587001513"></p>
<p>``Reuse at each level is defined as the number of times each data value is read from this level to its lower-cost levels during its lifetime`</p>
<p>比如a就是访问DRAM的次数</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1587001783.jpg" alt="1587001783"></p>
</li>
<li><p>Psum Accumulation Energy Cost:</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1587001817.jpg" alt="1587001817"></p>
<p>乘2的原因是psum需要累加的过程，需要进行写回，减1是因为access的一步骤在上面的过程中计算过了</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1587002031.jpg" alt="1587002031"></p>
</li>
</ul>
<p>最终的实验结果就是RS会是最好的，然后其他的架构总有这样那样的缺点。</p>
<h3 id="1-5-Summary"><a href="#1-5-Summary" class="headerlink" title="1.5 Summary"></a>1.5 Summary</h3><p>文章好像一直在说这个架构的有点，低能耗，低延时啊之类，但是我看他数据调度的时候使用了Folding（折叠）技术，感觉应该在速度上相比与其他架构可能会有劣势吧。</p>
<h2 id="Eyeriss-An-Energy-Efficient-Reconfigurable-Accelerator-for-Deep-Convolutional-Neural-Networks"><a href="#Eyeriss-An-Energy-Efficient-Reconfigurable-Accelerator-for-Deep-Convolutional-Neural-Networks" class="headerlink" title="Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks"></a>Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks</h2><p>JSSC</p>
<h3 id="1-1-What-the-problems-the-paper-mentioned-intro-1"><a href="#1-1-What-the-problems-the-paper-mentioned-intro-1" class="headerlink" title="1.1 What the problems the paper mentioned ?(intro)"></a>1.1 What the problems the paper mentioned ?(intro)</h3><p>相比与之前只是强调数据可重用以提高吞吐率，本篇文章新增了硬件可重配，以适应不同shape的cnn网络。同时针对之前的工作没有真正实现这样的一个系统的问题，本文还搭建了一个这样的系统进行测试。</p>
<h3 id="1-2-Summary-of-major-innovations-intro-1"><a href="#1-2-Summary-of-major-innovations-intro-1" class="headerlink" title="1.2 Summary of major innovations (intro)"></a>1.2 Summary of major innovations (intro)</h3><p>架构上有一定的创新，使用两个时钟，core clock为片上时钟，用于控制PE等计算单元，link clock用于控制DRAM以及总线，他们之间使用 异步通信。</p>
<p>使用RLC编码使得Relu引进的0不加载到global buffer中</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1587051197.jpg" alt="1587051197"></p>
<h3 id="1-3-What-are-some-intriguing-aspects-of-the-paper-design-amp-implementation-1"><a href="#1-3-What-are-some-intriguing-aspects-of-the-paper-design-amp-implementation-1" class="headerlink" title="1.3 What are some intriguing aspects of the paper?(design&amp;implementation)"></a>1.3 What are some intriguing aspects of the paper?(design&amp;implementation)</h3><ul>
<li><p>控制流</p>
<p>系统有两个控制器，</p>
<ul>
<li><p>The top-level control coordinates</p>
<p>DRAM与GLB之间的异步通信</p>
<p>使用Noc进行的GLB与PE Array之间的通信</p>
<p>控制RLC MODULE与Relu等模块</p>
</li>
<li><p>The lower-level control </p>
<p>控制的是PE内部的逻辑，使得每个PE可以独立的工作，而不需要像脉动整列那样。</p>
</li>
</ul>
<p>在运行CNN的一个layer之前，需要首先导入该层的配置文件，之后才加载数数据进行计算。</p>
</li>
<li><p>具体如何进行reuse</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1587023982.jpg" alt="1587023982"></p>
<p>详细可以参考<a href="https://zhuanlan.zhihu.com/p/58771811">https://zhuanlan.zhihu.com/p/58771811</a></p>
</li>
<li><p>RLC 编码</p>
<p>只对0使用RCL编码，这样就不需要二元对</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1587026006.jpg" alt="1587026006"></p>
<p>每64位编码三个不为0的数，最后一位指示字尾，每次Relu之后先编码再保存到DRAM中，这样节省read跟write的时间。</p>
</li>
<li><p>PE设计</p>
<p>PE设计成三级流水，第一级设计为取数，二、三级流水设计为计算</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1587034185.jpg" alt="1587034185"></p>
</li>
</ul>
<h3 id="1-4-How-to-test-compare-analyze-the-results-experiment-1"><a href="#1-4-How-to-test-compare-analyze-the-results-experiment-1" class="headerlink" title="1.4 How to test/compare/analyze the results?(experiment)"></a>1.4 How to test/compare/analyze the results?(experiment)</h3><p>总体来说，该芯片的面积存储单元占了2/3，计算单元仅仅占了7.4%</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1587034839.jpg" alt="1587034839"></p>
<p>实验表明，数据移动占用总能量消耗的45%，数据计算仅咱总能量消耗的10%</p>
<h3 id="1-5-Summary-1"><a href="#1-5-Summary-1" class="headerlink" title="1.5 Summary"></a>1.5 Summary</h3><p>这篇文章主要是将上一篇文章给的架构进行了实现，再实现过程中运用了一些小的细节算法，比如filter是如何映射到一维的，以及使用RCL编码对数据进行压缩等等</p>
<h2 id="Eyeriss-v2-A-Flexible-Accelerator-for-Emerging-Deep-Neural-Networks-on-Mobile-Devices"><a href="#Eyeriss-v2-A-Flexible-Accelerator-for-Emerging-Deep-Neural-Networks-on-Mobile-Devices" class="headerlink" title="Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices"></a>Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices</h2><h3 id="1-1-What-the-problems-the-paper-mentioned-intro-2"><a href="#1-1-What-the-problems-the-paper-mentioned-intro-2" class="headerlink" title="1.1 What the problems the paper mentioned ?(intro)"></a>1.1 What the problems the paper mentioned ?(intro)</h3><p>Eyeriss V2架构，主要是用来运行稀疏或则紧凑的神经网络的，使用一个叫做complementary approaches的方法，</p>
<h3 id="1-2-Summary-of-major-innovations-intro-2"><a href="#1-2-Summary-of-major-innovations-intro-2" class="headerlink" title="1.2 Summary of major innovations (intro)"></a>1.2 Summary of major innovations (intro)</h3><p>在V1的基础上，主要进行了2点改进，第一是引入了被称为hierarchical mesh的NOC结构，用来获取更好的数据和权重的重用性。其次是增加了对权重和数据的压缩处理，用来支持稀疏矩阵的运算。</p>
<h3 id="1-3-What-are-some-intriguing-aspects-of-the-paper-design-amp-implementation-2"><a href="#1-3-What-are-some-intriguing-aspects-of-the-paper-design-amp-implementation-2" class="headerlink" title="1.3 What are some intriguing aspects of the paper?(design&amp;implementation)"></a>1.3 What are some intriguing aspects of the paper?(design&amp;implementation)</h3><ul>
<li><p>再V1中，每个PE的存储空间是固定有限的，当超过限制之后，就得对GLB进行操作，增大延时。再V2中，将若干个GLB与PE组成一个cluster，然后通过router连接在2D mesh上，每个cluster都可以看作一个运算单元，内部包含多个PE</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1587050528.jpg" alt="1587050528"></p>
<p>这种存储集中式向分布式的转化也符合目前深度学习加速器的内存结构。由于深度学习算法的特点，较大的运算能够很好的切分为若干个小运算的组合，包含独立的数据和权重。eyeriss可以将切分好的运算单位分散到各个cluster中并行执行。由于eyeriss的基本运算结构是一维的卷积运算，卷积结果会累加在PE自身，并没有像systolic计算方式那样需要纵向传递，因此PE矩阵的大小并不会影响其运算效率</p>
</li>
<li><p>eyeriss V2通过每个cluster内部的issue router将数据根据当前运算的不同发送到对应的PE运算单元中去。针对卷积，全连接，depth-wise convolution的运算特点设计了multicast，broadcast和unicast等多种发射方式，不具有较好的可编程性。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1587051102.jpg" alt="1587051102"></p>
</li>
<li><p>当面对cluster间的数据传递，为了有效的进行数据复用，这个结构就更复杂了。为了CNN计算中权重的传递，设计了水平传输通道，每行的权重通过广播方式传输给每个PE。为了通道间数据可以进行累加，又设计了纵向的累加通道，每个PE的一维卷积结果通过纵向累加形成两维的卷积。为了上述灵活的网络结构，cluster设计了大量的router进行控制和数据分发。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1587051197.jpg" alt="1587051197"></p>
</li>
</ul>
<h3 id="1-4-Summary"><a href="#1-4-Summary" class="headerlink" title="1.4 Summary"></a>1.4 Summary</h3><p>总体来说，eyeriss V2相比于V1，从单一的核内矩阵结构过渡到类似于众核的NOC结构，NOC是一个比较成熟的设计，对于深度学习的应用需要着重考虑数据带宽的需求。对于其复杂的数据分发方式，可以作为参考，但并不是一个很好的实现方案。</p>
