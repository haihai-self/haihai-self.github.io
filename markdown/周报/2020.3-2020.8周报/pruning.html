<p>[TOC]</p>
<h2 id="Learning-both-Weights-and-Connections-for-Efficient-Neural-Networks"><a href="#Learning-both-Weights-and-Connections-for-Efficient-Neural-Networks" class="headerlink" title="Learning both Weights and Connections for Efficient Neural Networks"></a>Learning both Weights and Connections for Efficient Neural Networks</h2><p>han song NIPS 2015</p>
<h3 id="1-1-What-the-problems-the-paper-mentioned-intro"><a href="#1-1-What-the-problems-the-paper-mentioned-intro" class="headerlink" title="1.1 What the problems the paper mentioned ?(intro)"></a>1.1 What the problems the paper mentioned ?(intro)</h3><p>本文可以得上是deep compression:的开山之作，在移动设备上运行一个完整的大型DNN网络几乎是不可能的，比如运行一个十亿个连接的DNN，仅仅是DRAM的访问功耗就达到了12.8W，通过剪枝量化的方法，在不损失精度的情况下，将网络大小（AlexNet, VGG）等网络的大小压缩了10倍左右。</p>
<h3 id="1-2-Summary-of-major-innovations-intro"><a href="#1-2-Summary-of-major-innovations-intro" class="headerlink" title="1.2 Summary of major innovations (intro)"></a>1.2 Summary of major innovations (intro)</h3><p>提出了一种剪枝的方法，主要通过三个步骤进行网络压缩</p>
<ul>
<li><p>训练网络找到重要的连接（简单的认为权值大的为重要）</p>
</li>
<li><p>丢掉不重要的连接</p>
</li>
<li><p>重新训练网络</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584887362.jpg" alt="1584887362"></p>
</li>
</ul>
<h3 id="1-3-How-about-the-important-related-works-papers-related-work"><a href="#1-3-How-about-the-important-related-works-papers-related-work" class="headerlink" title="1.3 How about the important related works/papers?(related work)"></a>1.3 How about the important related works/papers?(related work)</h3><p>Compressingneural  networks with the hashing trick arXiv preprint arXiv:1504.04788, 2015. 主要是讲了剪枝之后如何存储权重矩阵，使得DNN高效的运行。</p>
<h3 id="1-4-What-are-some-intriguing-aspects-of-the-paper-design-amp-implementation"><a href="#1-4-What-are-some-intriguing-aspects-of-the-paper-design-amp-implementation" class="headerlink" title="1.4 What are some intriguing aspects of the paper?(design&amp;implementation)"></a>1.4 What are some intriguing aspects of the paper?(design&amp;implementation)</h3><p>使用L1正则化与L2正则化得到的结果将会不同，L1正则化在retrain之前的accuracy比较高，但是retrain之后的结果没有L2正则化得到的效果好。</p>
<ul>
<li><p>L1正则化是指权值向量www中各个元素的绝对值之和</p>
<p>可以产生稀疏矩阵，即结果中接近0的权值更多。</p>
</li>
<li><p>L2正则化指的是权值求平方和之后再求平方更</p>
<p>L2正则化可以防止过拟合</p>
</li>
</ul>
<p>节点所有的权重连接删除之后，该节点也需要进行删除</p>
<h3 id="1-5-How-to-test-compare-analyze-the-results-experiment"><a href="#1-5-How-to-test-compare-analyze-the-results-experiment" class="headerlink" title="1.5 How to test/compare/analyze the results?(experiment)"></a>1.5 How to test/compare/analyze the results?(experiment)</h3><p>表中从左到右的分别是原始weight的数量，原来需要浮点运算的次数，原始weight中非0的数量，剪枝之后非0的数量，实际浮点操作的百分比，可以看到，剪枝掉的大部分还是fc的权重，对于卷积操作的权重剪掉的很少。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584889300.jpg" alt="1584889300"></p>
<p>最终得到的是非结构化的网络，无法在通用GPU上进行加速</p>
<h2 id="Deep-compression-compressing-deep-neural-networks-with-pruning-trained-quantization-and-Huffman-Coding"><a href="#Deep-compression-compressing-deep-neural-networks-with-pruning-trained-quantization-and-Huffman-Coding" class="headerlink" title="Deep compression: compressing deep neural networks with pruning, trained quantization and Huffman Coding"></a>Deep compression: compressing deep neural networks with pruning, trained quantization and Huffman Coding</h2><p>han song ICLR 2016</p>
<h3 id="1-1-What-the-problems-the-paper-mentioned-intro-1"><a href="#1-1-What-the-problems-the-paper-mentioned-intro-1" class="headerlink" title="1.1 What the problems the paper mentioned ?(intro)"></a>1.1 What the problems the paper mentioned ?(intro)</h3><p>在上一篇文章的基础上加上了量化，将AlexNet，VGG分别压缩了35倍、49倍，分别加速了3倍、4倍，节能了3倍、7倍。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584946519.jpg" alt="1584946519"></p>
<p>首先通过剪枝的方法，将网络压缩到原来的9-13倍，之后通过量化的方法，将网络压缩到原来的27-31倍，在通过哈夫曼编码的方法，最终将网络压缩到原来的35-49倍。</p>
<h3 id="1-2-Summary-of-major-innovations-intro-1"><a href="#1-2-Summary-of-major-innovations-intro-1" class="headerlink" title="1.2 Summary of major innovations (intro)"></a>1.2 Summary of major innovations (intro)</h3><p>将剪枝跟量化结合再了一起，并使用哈夫曼编码，发现了剪枝跟量化之间互不影响的关系，使得weight得到了极大的压缩。</p>
<h3 id="1-3-How-about-the-important-related-works-papers-related-work-1"><a href="#1-3-How-about-the-important-related-works-papers-related-work-1" class="headerlink" title="1.3 How about the important related works/papers?(related work)"></a>1.3 How about the important related works/papers?(related work)</h3><p>从1.1中图可以看到，首先使用剪枝的方法不断的retrain得到一个网络结构，然后再将这个网络结构量化，量化的的过程如下所示。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584969937.jpg" alt="1584969937"></p>
<p>上图解释了量化的过程，使用的k-means聚类方法，将左上weight矩阵分成4类，然后将不同的类别使用聚类中心表示，原矩阵中只保存类别的索引。在<code>权值更新</code>的时候，所有的gradients按照weight矩阵的颜色来分组，同一组的gradient做一个相加的操作，得到是sum乘上learning rate再减去共享的centroids，得到一个fine-tuned centroids。当一次训练完成之后，再重新使用聚类算法，重复以上过程。</p>
<p>再量化到想要的结果之后，再对结果进行哈夫曼编码，又可以进一步压缩weight</p>
<p>通过以上的三个步骤，就可以将weight压缩带原来的40倍左右。</p>
<h3 id="1-4-How-to-test-compare-analyze-the-results-experiment"><a href="#1-4-How-to-test-compare-analyze-the-results-experiment" class="headerlink" title="1.4 How to test/compare/analyze the results?(experiment)"></a>1.4 How to test/compare/analyze the results?(experiment)</h3><p>该文章评价的比较全面，首先是准确率与压缩比方面。</p>
<ul>
<li><p>再不损失精度的情况下，weight最多可以压缩49倍</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584970761.jpg" alt="1584970761"></p>
</li>
<li><p>比较又意思的是，他对conv与fc使用了不同的精度，并且比较了不同情况下的压缩比，P表示只是用剪枝，P+Q表示同时使用剪枝与量化，P+Q+H表示同时使用三种技术，结果如下图所示。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584970946.jpg" alt="1584970946"></p>
</li>
<li><p>在只是用pruning或者quantization的情况下，压缩的原来的8%的情况之下，精度就开始急剧下降，当时同时使用两种技术的情况下，可以压缩的原来的2%左右才会导致精度的急剧下降。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584971094.jpg" alt="1584971094"></p>
</li>
<li><p>下面证明了剪枝与量化之间并不会相互的影响，在原来网络上使用量化技术也不会导致精度损失，在剪枝的结果上进行量化与只使用剪枝的网络达到的是同样的效果。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584971285.jpg" alt="1584971285"></p>
</li>
<li><p>在量化的过程中，聚类中心的确定会对结果造成较大的影响，综合来看，使用均匀分布模型得到量化中心产生的结果最好。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584971469.jpg" alt="1584971469"></p>
</li>
</ul>
<h3 id="1-5-Summary"><a href="#1-5-Summary" class="headerlink" title="1.5 Summary"></a>1.5 Summary</h3><p>这是一篇对非结构化剪枝比较详细的论文，与近似计算是两种不同的技术，量化首先就是不断的训练weight到指定的bits，不存在近似这一说，其次当时我们使用的网络是一个完全的网络，存在大量的冗余，我觉得还是得在剪枝后的网络上进行研究比较有意义。</p>
<h2 id="Dynamic-Network-Surgery-for-Efficient-DNNs"><a href="#Dynamic-Network-Surgery-for-Efficient-DNNs" class="headerlink" title="Dynamic Network Surgery for Efficient DNNs"></a>Dynamic Network Surgery for Efficient DNNs</h2><p>NIPS 2016 <a href="http://arxiv.org/abs/1608.04493">http://arxiv.org/abs/1608.04493</a></p>
<h3 id="1-1-What-the-problems-the-paper-mentioned-intro-2"><a href="#1-1-What-the-problems-the-paper-mentioned-intro-2" class="headerlink" title="1.1 What the problems the paper mentioned ?(intro)"></a>1.1 What the problems the paper mentioned ?(intro)</h3><p>han song 的工作简单的认为权重小的weight是不重要的，这其实是有问题的，而且剪枝掉的weight与节点可能是错误的剪枝，在这个基础上，作者提出了一种叫做dynamic network的网络，优化了这些问题。实现了LeNet-5 and AlexNet by a factor of 108× and 17.7× respectively</p>
<h3 id="1-2-Summary-of-major-innovations-intro-2"><a href="#1-2-Summary-of-major-innovations-intro-2" class="headerlink" title="1.2 Summary of major innovations (intro)"></a>1.2 Summary of major innovations (intro)</h3><p>剪枝或者不正确的剪枝会导致精度的下降，为了解决这样的问题，加入了反悔的机制，将剪枝掉的一部分weight加入下一轮的训练，一旦发现这些weight是重要的话，将这些weight从新加入网络</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584892718.jpg" alt="1584892718"></p>
<h3 id="1-3-What-are-some-intriguing-aspects-of-the-paper-design-amp-implementation"><a href="#1-3-What-are-some-intriguing-aspects-of-the-paper-design-amp-implementation" class="headerlink" title="1.3 What are some intriguing aspects of the paper?(design&amp;implementation)"></a>1.3 What are some intriguing aspects of the paper?(design&amp;implementation)</h3><p>里面有句话引用一下<code>However, the parameter importance (i.e., the connection importance) in a certain network is extremely difficult to measure because of the mutual influences and mutual activations among interconnected neurons. </code></p>
<p>由于神经网络的复杂性，因为有其他的连接，有些节点可能是多余的，但是一旦删除了其他的节点，这些节点就成了关键节点。</p>
<ul>
<li>Hadamard product，矩阵的内积，就两个同型矩阵对应元素相乘。</li>
</ul>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584929825.jpg" alt="1584929825"></p>
<p>$L() $表示的是网络的损失函数，Tk表示与Wk同型的矩阵，但是其中的元素只有0，1。0表示删除，1表示在当前这轮迭代是重要的，两个矩阵做内积，对应Tk为0的地方就不保留了。h<sub>k</sub>表示Wk的连接是否关键。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584930774.jpg" alt="1584930774"></p>
<p>使用梯度下降与拉格朗日乘子法对W进行更新，h<sub>k</sub>函数更新Tk如下所示。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584931061.jpg" alt="1584931061"></p>
<p>作者还举了一个简单的例子说明情况，如下图所示是一个XOR的网络，T1表示剪枝的情况，白色的表示一直没有没剪枝的weight，绿色的表示被错误剪枝之后加回来的weight，黑色的表示被剪枝掉的weight。值得令人注意的是，图b中，可以看到weight矩阵的第一行与最后一行的值相近，说明两个神经元的功能类似那个，在剪枝的过程中，很好的剪掉了功能类似的神经元，说明工作还是work的。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584931798.jpg" alt="1584931798"></p>
<h3 id="1-4-How-to-test-compare-analyze-the-results-experiment-1"><a href="#1-4-How-to-test-compare-analyze-the-results-experiment-1" class="headerlink" title="1.4 How to test/compare/analyze the results?(experiment)"></a>1.4 How to test/compare/analyze the results?(experiment)</h3><p>比较有意思的是他对比了han song’s work训练出的参数与用他们的方法训练出的参数个数，可以看到，相对与han song 方法参数规模要小很多，可以得出的结论就是错误的剪枝会对网络的准确率照成很大影响</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584932351.jpg" alt="1584932351"></p>
<h3 id="1-5-Summary-1"><a href="#1-5-Summary-1" class="headerlink" title="1.5 Summary"></a>1.5 Summary</h3><p>这还是非结构化的剪枝，对硬件的还是不友好。虽然说重新定义了什么是重要的权重，但是的话总感觉使用的方法太简单了，还是简单的认为权重大的比较重要，感觉神经元之间的相互关系还是难以度量，应该使用更加复杂更加系统的方法。</p>
<p>文章还说明了weight之间的相互影响关系，让我觉得误差注入的方法更加复杂，感觉不同的地方肯定是需要不同的误差，想要找到理论化的办法或许很困难，或许需要遍历搜索的方法。</p>
<h2 id="Pruning-filters-for-efficient-convnet"><a href="#Pruning-filters-for-efficient-convnet" class="headerlink" title="Pruning filters for efficient convnet"></a>Pruning filters for efficient convnet</h2><p>ICLR 2017</p>
<h3 id="1-1-What-the-problems-the-paper-mentioned-intro-3"><a href="#1-1-What-the-problems-the-paper-mentioned-intro-3" class="headerlink" title="1.1 What the problems the paper mentioned ?(intro)"></a>1.1 What the problems the paper mentioned ?(intro)</h3><p>文章提出了结构化剪枝的方法，han song的剪枝工作中，引入了稀疏矩阵，导致矩阵的结构不在规整，不方便使用现在的加速硬件GPU等对网络进行加速，并且修剪的大多数都是fc层的连接，这些连接在网络中往往只是占很小的一部分计算量。</p>
<p>文章提出了一种对filter剪枝的方法，由于convolutional layer往往占了网络中很大一部分的计算量，对这部分进行剪枝会导致计算量大幅的减少。</p>
<h3 id="1-2-Summary-of-major-innovations-intro-3"><a href="#1-2-Summary-of-major-innovations-intro-3" class="headerlink" title="1.2 Summary of major innovations (intro)"></a>1.2 Summary of major innovations (intro)</h3><p>文章介绍了filter pruning的方法，为Restnet中convolutional layer进行了灵敏度分析。</p>
<p>如下图所示，将一个卷积核剪掉，对应的一个feature map就没了，当前层的计算减少了，同时减少了输出，进一步影响了下层卷积的计算。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1584978515.jpg" alt="1584978515"></p>
<h3 id="1-3-What-are-some-intriguing-aspects-of-the-paper-design-amp-implementation-1"><a href="#1-3-What-are-some-intriguing-aspects-of-the-paper-design-amp-implementation-1" class="headerlink" title="1.3 What are some intriguing aspects of the paper?(design&amp;implementation)"></a>1.3 What are some intriguing aspects of the paper?(design&amp;implementation)</h3><ul>
<li><p>修剪Filter步骤：</p>
<ol>
<li>计算 Filter 中所有权值的绝对值之和</li>
<li>根据求和大小排列 Filter</li>
<li>删除数值较小的 Filter （权重数值越小，代表权重的重要性越弱）</li>
<li>对删除之后的 Filter 重新组合，生成新的Filter矩阵</li>
</ol>
</li>
<li><p>剪枝的敏感度</p>
<p>对每一层进行单独剪枝，然后看验证集上的精度变化。</p>
<p>对于VGG-16，一些卷积层的敏感度差不多，使用相同比例的剪枝数量，要是对剪枝比较敏感的卷积层，是用比例相对较小的剪枝。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1585531527.jpg" alt="1585531527"></p>
</li>
<li><p>多层同时修剪</p>
<ul>
<li>独立修剪：每层是独立的</li>
<li>贪心修剪：修建时考虑上一层修剪的内容</li>
</ul>
<p>两种方法的区别：独立修剪在计算（求权重绝对值之和）时不考虑上一层的修剪情况，所以计算时下图中的黄点仍然参与计算；贪心修剪计算时不计算已经修剪过的，即黄点不参与计算。结果证明第二种方法精度更高。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1585492488.jpg" alt="1585492488"></p>
<p>全局修剪是必要的，独立修剪会导致三个问题</p>
<ul>
<li>独立修剪收敛很慢，计算资源消耗巨大，且没有必要</li>
<li>全局修剪会给你一个全局的概念，会导致一个较小的网络。</li>
<li>对于复杂的网络，全局剪枝是必要的。</li>
</ul>
</li>
<li><p>残差网络的处理</p>
<p>待了解Restnet</p>
</li>
<li><p>再训练</p>
<ul>
<li><p>一次修剪多filter或者多layer再重新训练</p>
<p>这种方法收敛较快，但是一旦修剪得比较多或者修剪到了敏感层的话，可能导致精度无法恢复</p>
</li>
<li><p>一次修剪一次，并迭代的进行</p>
<p>这种方法会得到较好的结果，但是的话迭代的次数很多。</p>
</li>
</ul>
</li>
</ul>
<h3 id="1-4-How-to-test-compare-analyze-the-results-experiment-2"><a href="#1-4-How-to-test-compare-analyze-the-results-experiment-2" class="headerlink" title="1.4 How to test/compare/analyze the results?(experiment)"></a>1.4 How to test/compare/analyze the results?(experiment)</h3><p>值得注意的是他还使用了scratch-train，就是剪枝得到了网络结构之后，再使用这样的网络结构进行从新初始化进行训练，结果比使用训练好的结果然后进行剪枝差，pruned-A，B，C是跳过不同的卷积层进行剪枝，为了是验证不同的卷积层有着不同铭感度的结果，他们发现，越深的层的敏感度越高。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1585534798.jpg" alt="1585534798"></p>
<h3 id="1-5-Summary-2"><a href="#1-5-Summary-2" class="headerlink" title="1.5 Summary"></a>1.5 Summary</h3><p>这种方法需要手工精细的设计剪枝结构，即选择每层应该如何剪枝，不自动化，工作量应该挺大，且感觉可以加上之前文章所说的，就是要是发现剪枝掉的卷继承依然是重要的，需要加回来进行重新训练网络结构。</p>
<h2 id="Continuous-control-with-deep-reinforcement-learning"><a href="#Continuous-control-with-deep-reinforcement-learning" class="headerlink" title="Continuous control with deep reinforcement learning"></a>Continuous control with deep reinforcement learning</h2><p>ICLR 2016</p>
<h3 id="1-0-补充资料"><a href="#1-0-补充资料" class="headerlink" title="1.0 补充资料"></a>1.0 补充资料</h3><ul>
<li><p>Q-learning</p>
<p>参考<a href="https://link.zhihu.com/?target=http://mnemstudio.org/path-finding-q-learning-tutorial.htm">q-learning-tutorial</a></p>
<p>如下图所示，需要从2房间走到5房间</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/modeling_environment_clip_image002a.gif" alt="modeling_environment_clip_image002a"></p>
<p>将上图转化为一个叫做R矩阵的东西，同时也叫做环境，矩阵的行代表当前所处的状态，列代表能达到的状态，其中0表示能达到没有奖励，100表示能达到目的地点，-1表示不连通。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/r_matrix1.gif" alt="r_matrix1"></p>
<p>现在我们添加一个类似“矩阵R”的“矩阵Q”作为Agent的大脑，即通过经验学到的东西。“矩阵Q”的行表示Agent当前的状态，列表示下一个状态（节点之间的链接）的可能动作。初始化为0。</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/q_matrix1.gif" alt="q_matrix1"></p>
<p>Q矩阵的跟新规则：<code>Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]</code></p>
<ol>
<li><p>假设我们随机选择状态1，根据R矩阵，状态5能到达的下一个状态为状态3、5，假设下一个状态为状态5，则根据Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]跟新Q矩阵为Q(1, 5) = R(1, 5) + 0.8 * Max[Q(5, 1), Q(5, 4), Q(5, 5)] = 100 + 0.8 * 0 = 100</p>
<p><img src="http://mnemstudio.org/ai/path/images/q_matrix2.gif" alt="img"></p>
</li>
<li><p>再次随机选择一个状态，假设为状态3，根据R矩阵，状态3能达到的下一个状态分别为1、2、4，我们假设下一个状态为1，根据规则Q(3, 1) = R(3, 1) + 0.8 * Max[Q(1, :))] = 0 + 0.8 * 100= 80</p>
<p><img src="http://mnemstudio.org/ai/path/images/q_matrix3.gif" alt="img"></p>
<p>算法伪代码如下所示</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/881bb2c39cb54eb1414ce25c7ceddd4.png" alt="881bb2c39cb54eb1414ce25c7ceddd4"></p>
</li>
</ol>
</li>
<li><p>sarsa算法</p>
<p><img src="https://homework-image.oss-cn-beijing.aliyuncs.com/image/1585906689.jpg" alt="1585906689"></p>
<p>sarsa算法与Q-learning算法最大的不同之处在于，sarsa将下一个状态采取的动作做为Q现实，而Q-learning则是将下一个状态最大的可能的动作作为Q现实，但是到下一个状态的时候，还不一定选择他。</p>
</li>
<li><p>sarsa(lambda)</p>
<p>在sarsa算法中，没有达到终点之前，所有的值都是0，且就算达到了终点，也只是有距离到达终点的那一步，更新为不为0的值。</p>
<p>Sarsa(lambda)则使用了一个E矩阵，保存之前所走的每一步，在更新的时候，会对之前的每一步进行更新，且越接近当前步的权值越大，这样当到达终点的时候，之前的所有的步都会更新得到一个值，而不是全为0了。</p>
<p>算法描述：</p>
<p><img src="https://morvanzhou.github.io/static/results/reinforcement-learning/3-3-1.png" alt="img"></p>
</li>
</ul>
<ul>
<li><p>Deep-Q-Learning</p>
<p>q-table存在一个问题，真实情况的state可能无穷多，这样q-table就会无限大，解决这个问题的办法是通过神经网络实现q-table。输入state，输出不同action的q-value</p>
</li>
</ul>
<h3 id="1-1-What-the-problems-the-paper-mentioned-intro-4"><a href="#1-1-What-the-problems-the-paper-mentioned-intro-4" class="headerlink" title="1.1 What the problems the paper mentioned ?(intro)"></a>1.1 What the problems the paper mentioned ?(intro)</h3><h3 id="1-2-Summary-of-major-innovations-intro-4"><a href="#1-2-Summary-of-major-innovations-intro-4" class="headerlink" title="1.2 Summary of major innovations (intro)"></a>1.2 Summary of major innovations (intro)</h3><h3 id="1-3-What-are-some-intriguing-aspects-of-the-paper-design-amp-implementation-2"><a href="#1-3-What-are-some-intriguing-aspects-of-the-paper-design-amp-implementation-2" class="headerlink" title="1.3 What are some intriguing aspects of the paper?(design&amp;implementation)"></a>1.3 What are some intriguing aspects of the paper?(design&amp;implementation)</h3><h3 id="1-4-How-to-test-compare-analyze-the-results-experiment-3"><a href="#1-4-How-to-test-compare-analyze-the-results-experiment-3" class="headerlink" title="1.4 How to test/compare/analyze the results?(experiment)"></a>1.4 How to test/compare/analyze the results?(experiment)</h3><h3 id="1-5-Summary-3"><a href="#1-5-Summary-3" class="headerlink" title="1.5 Summary"></a>1.5 Summary</h3><h2 id="AMC-Automl-for-model-compression-and-acceleration-on-mobile-devices"><a href="#AMC-Automl-for-model-compression-and-acceleration-on-mobile-devices" class="headerlink" title="AMC: Automl for model compression and acceleration on mobile devices"></a>AMC: Automl for model compression and acceleration on mobile devices</h2><p>ECCV 2018</p>
<h3 id="1-1-What-the-problems-the-paper-mentioned-intro-5"><a href="#1-1-What-the-problems-the-paper-mentioned-intro-5" class="headerlink" title="1.1 What the problems the paper mentioned ?(intro)"></a>1.1 What the problems the paper mentioned ?(intro)</h3><p>propose AutoML for Model Compres-sion, achieved 2.7% better accuracy than the hand-crafted model compression policy for VGG-16 on ImageNet. 使用启发式的剪枝方法通常是sub-optimal，使用手动剪枝的方法通常会花费大量的时间。作者还将所做的工作真正的移植到了Android手机上</p>
<p>启发式的剪枝方法通常会有一些规则</p>
<ul>
<li>在抽取低级特征且参数最少的第一层中剪掉更少的参数</li>
<li>尽可能的再fc层修剪，因为这里所拥有的参数数量是最多的</li>
<li>对于剪枝敏感的层也要进行尽可能少的修剪。</li>
</ul>
<p>构建了两种压缩模型。</p>
<ul>
<li>resource-constrained对应的是对延迟要求很高的应用</li>
<li>accuracy-guaranteed对应的是对精度要求很高，但是对延时要求没有那么高的应用</li>
</ul>
<h3 id="1-2-Summary-of-major-innovations-intro-5"><a href="#1-2-Summary-of-major-innovations-intro-5" class="headerlink" title="1.2 Summary of major innovations (intro)"></a>1.2 Summary of major innovations (intro)</h3><p>利用强化学习自动采样设计空间并提高压缩模型的质量，且对所有的隐藏层微调完成之后，才进行retrain，这样极大的提高了训练的效率，节省了搜索的时间。</p>
